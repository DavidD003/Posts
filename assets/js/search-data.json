{
  
    
        "post0": {
            "title": "Deploying A Schedule Building Algorithm",
            "content": "Context . Goal . Deploy an algorithm for schedulers to use that is: . easy to use and learn | transparent | quick | flexible | . Motivation . In a 24/7 manufacturing environment, the weekend shifts are covered mostly by overtime, which is scheduled according to employee availability, subject to constraints outlined in the labour collective agreement. Due to changing production needs as well as staff availability, the schedule must be re-drafted many times, often on short notice and under tight time constraints. Drafting it is tedious, error-prone, and time consuming. It could be automated. . Challenges . -Data being manually entered in a variety of formats or not available in machine readable form. (e.g. total hours, employee type, employee availability, individualized job restrictions, outlier reasons for non-eligibility such as consecutive days worked) This is probably the main challenge!! . Algorithm ambiguity. The collective agreement defines the constraints that each assignment decision is subject to, but doesn&#39;t strictly specify all aspects, allowing for arbitrary choice on schedulers part | Many esoteric rules and edge cases around assignments being valid or not, which are also subject to change at time of contract renegotiation. | Usability. The deployment must be available to all schedulers, and have a very low barrier to entry w.r.t. training and usability. | . A Bit of Lore . The notion of automating the process solution has been bouncing around my head for over a year. I always felt the main challenge was the situation posed by the data... the bad formatting relegated to excel sheets, necessarily made that way from human input and usage modality not being the same as what is best for machine readability. I am very confident I could make something that worked in VBA, but the nature of that language makes it such a pain to develop with, particularly with bad formatting. I knew python was a better solution, but didn&#39;t have the bridge between the two to make something that worked. Finally when following the FastAI course I came across the HFS+Gradio wombo combo for sharing python scripts publicly via a great UI. This discovery got me to finally choose to commit to that solution path . Solution . Features . The program should take in various required inputs and return a completed weekend staffing schedule, with a separate text output with the sequence of assignments made. Facilitated by the Blocks functionality within Gradio/HFS will be the possibility of feeding in an assignment number and returning a partially completed schedule at that step. . Codebase . Using Gradio hosted via HFS for making a python algorithm available with easy integration of inputs and outputs. At first blush I thought that Pandas DataFrames would be the best input mode for tabular data, but ruled that out when HFS didn&#39;t allow for bulk copying and pasting. Maybe that was for the best because this pushed me to figure out how to work with the generic File input/output mode. It might be a little more painful to program (have to define methods to identify the right tables within the excel file), but a lot nice on the end user experience (drag and drop relevant files and go!). I was concerned about the extra steps of processing the excel file, but as with everything Python there is a library for that! I started off with some basic tests to ensure that what I wanted/needed to do was possible. . File Manipulation . Here is my proof of concept for File manipulation. If copied into an empty Gradio space on HFS, it takes in an excel file, and adds a new table to the spreadsheet. This was all i needed to know that this could be done... . import gradio as gr import openpyxl as pyxl #openPyXl allows for excel file manipulation in python def myFunction(fl,txt): myWb=pyxl.load_workbook(fl.name) #Load excel file tab = pyxl.worksheet.table.Table(displayName=&quot;Table3&quot;, ref=&quot;E1:F5&quot;) #Define new table style = pyxl.worksheet.table.TableStyleInfo(name=&quot;TableStyleMedium9&quot;,showRowStripes=True, showColumnStripes=True) tab.tableStyleInfo = style #Assign style to table ws=myWb.active ws.add_table(tab) #Add defined table to sheet within the loaded workbook otpt_fl_name=&#39;try.xlsx&#39; myWb.save(otpt_fl_name) #Save file return otpt_fl_name #Define output for HFS interface demo = gr.Interface( myFunction, #Func to take in file and text [ gr.File( ), gr.Textbox( label=&quot;Initial text&quot;, lines=3, value=&quot;The quick brown fox jumped over the lazy dogs.&quot;, ), ], gr.File(), description=&quot;Enter refusal files&quot;, ) demo.launch() . Retrieving Disparate Tables . As mentioned previously, one challenge would be to pull data from tables scattered in an unpredictable way throughout the sheet. Here I had to remember that sometimes the easiest way to rob a bank is through the front door, not trying to break through the wall... I simply changed the existing excel template files (filled in by end user) so that the data tables were actually defined as &#39;Tables&#39; by excel... this made them reference-able by the openPyXl tools. Some further data type transformations were required. Example with a blank book containing a trivial data table called &#39;tstTbl&#39; in Excel: . import openpyxl as pyxl import pandas as pd import numpy as np myWb=pyxl.load_workbook(&#39;../images/Other_Files/TblTestBook.xlsx&#39;) #Didn&#39;t think the .. parent directory would work but it does! ws=myWb[&#39;Sheet1&#39;] tab=ws.tables[&#39;tstTbl&#39;] #Pull out table def tbl_to_df(tab): ref=tab.ref #Pull cell reference to string for display tab=[[x.value for x in sublist] for sublist in ws[tab.ref]] #Convert to list of lists (each sublist as row of excel table) return pd.DataFrame(tab) #Convert nested lists to Dataframe print(tbl_to_df(tab)) . 0 1 2 0 myHead1 myHead2 myHead3 1 1 a . 2 2 b , 3 3 c ] . And pulling info from multiple tables in a sheet . for t in ws.tables: tab=ws.tables[t] tab=tbl_to_df(tab) print(&#39;Table cells reference is &quot;&#39;+str(ref)+&#39;&quot;:&#39;) print(tab) print(&#39;&#39;) . Table cells reference is &#34;A9&#34;: 0 1 2 0 myHead1 myHead2 myHead3 1 1 a . 2 2 b , 3 3 c ] Table cells reference is &#34;A9&#34;: 0 1 0 Names Hours 1 Alice 4 2 Bob 20 3 Clark 8 4 Dave 15 Table cells reference is &#34;A9&#34;: 0 1 0 Names Hours 1 Arnold 4 2 Bill 60 3 Charles 53 4 Dick 10 Table cells reference is &#34;A9&#34;: 0 1 0 Names Hours 1 Arthur 24 2 Blaire 70 3 Chuck 22 4 Darryl 12 . At this point I can say I am constantly resisting the urge to just run away with the coding! Trying to enforce a best practice of starting off with creating not just an abstract understanding of the problem, but a particular and specified framework in which I am operating, that is, figuring out the specific nature of the inputs I will have before I go nuts building my tower of babel! Next is to mock up a way to retrieve data when a worksheet has a single &#39;table&#39; not defined in Excel. That is, manually entered data in a tabular format that due to legacy sheet formatting is not able to be defined as a native Excel Table, precluding the use of table indexing seen in the previous example... My approach assumes a known top left cell, and knowing in my framework that only certain columns will be required here. . ws=myWb[&#39;Arb_Tbl&#39;] df = pd.DataFrame(ws.values) df . 0 1 2 3 4 5 6 7 8 . 0 None | None | None | None | None | None | None | None | None | . 1 None | None | None | None | None | None | None | None | None | . 2 Name | id | Attr1 | Attr2 | Attr3 | Attr4 | Attr5 | Attr6 | Attr7 | . 3 Bob Back | 0 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | . 4 Jeff Jahl | 1 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | . 5 Hodge Hoss | 2 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | . 6 Kev Kroll | 3 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | . 7 Tim Tin | 4 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | =RAND()*5 | . Above we can see the loose table in the wild... ws.values pulls the whole sheet, which would be good, except it grabs formulas. Unfortunately, per the docs, openPyXl will never evaluate formulas! Time to work smart, not hard. I choose to simply use the &#39;mouse wriggle&#39; technique shared at this webpage to manually convert formulas to values before passing my workbook into my functions. Though it is sad that the user experience won&#39;t be as smooth as dragging and dropping files. . myWb=pyxl.load_workbook(&#39;../images/Other_Files/TblAsValues.xlsx&#39;) ws=myWb[&#39;Arb_Tbl&#39;] #1st find bottom row with data for i in range(3,200): #Loop up to arbitrary number, prefer to have defined end for infinte loop stopgap ref=&quot;A&quot;+str(i) if ws[ref].internal_value==None: #Condition met when end of data found. btmRow=i-1 break tab=[[x.internal_value for x in sublist] for sublist in ws[&#39;A3:I&#39;+str(btmRow)]] df_IdNameHours=pd.DataFrame(tab) #Assuming column I is end of useful data print(df_IdNameHours) . 0 1 2 3 4 5 6 7 0 Name id Attr1 Attr2 Attr3 Attr4 Attr5 Attr6 1 Bob Back 0 0.666338 0.778057 4.822291 1.632892 2.374052 2.922761 2 Jeff Jahl 1 3.865002 4.639129 4.989458 3.552441 0.809309 0.61727 3 Hodge Hoss 2 3.088418 0.360548 0.416228 1.700045 3.059734 0.443541 4 Kev Kroll 3 3.466984 3.967881 2.390047 0.40636 4.68963 0.376537 5 Tim Tin 4 2.910975 1.203692 0.146427 1.411585 0.963408 0.467773 8 0 Attr7 1 3.91101 2 4.264042 3 4.720742 4 1.797408 5 0.369525 . Of course, the actual code deployed is more complex than this... In particular is the case of converting a data table indicating who is trained on what in human readable form to one that is more machine readable. The existing process has a table with one row for each staff person, with one column for each job, and a 1 or 0 if the person is trained or not . ws=myWb[&#39;Skills_Matrix&#39;] dataArr=np.array(pd.DataFrame(ws.values)) #Convert data table (skill matrix format) into data table (skills record format) skills=[] #Initiate new container for individual in dataArr[1:]: #iterate over all data rows for skl in range(1,len(individual)): #iterate over indices not containing the name if individual[skl]==1: skills.append([individual[0],dataArr[0][skl]]) dataArr=pd.DataFrame(skills) print(tbl_to_df(ws.tables[&#39;Skills_Mtx&#39;])) print(&#39;&#39;) print(dataArr) . 0 1 2 3 4 5 0 Column1 Brew Filter Pack Ship Manage 1 Alfred 1 0 0 1 0 2 Bill 0 1 0 0 1 3 Chris 1 1 1 0 0 4 Dante 1 0 0 0 1 5 Edgar 0 0 1 1 0 0 1 0 Alfred Brew 1 Alfred Ship 2 Bill Filter 3 Bill Manage 4 Chris Brew 5 Chris Filter 6 Chris Pack 7 Dante Brew 8 Dante Manage 9 Edgar Pack 10 Edgar Ship . Now that data tables are removed from excel, we need a means of filtering/sorting them. Unfortunately I could not find good means to do this with existing tools (numpy arrays, pandas DataFrames). Fortunately, this meant learning something new! Here I bring in sqlite3, which allows for running a sql table locally. SQL is Structured Query Language, a language and toolset all about tabular data. It will allow us to do any sort of filter, sort, or view of a table that we could want. The following is a little sample taken modified from [the docs] (https://docs.python.org/2/library/sqlite3.html) . import sqlite3 conn = sqlite3.connect(&#39;example.db&#39;) c = conn.cursor() c.execute(&#39;&#39;&#39;CREATE TABLE IF NOT EXISTS stocks (date text, trans text, symbol text, qty, price)&#39;&#39;&#39;) # Create table. c.execute(&#39;&#39;&#39;DELETE FROM stocks&#39;&#39;&#39;) #if table already existed, will have data... delete existing data to refresh with new. purchases = [[&#39;2006-03-28&#39;, &#39;BUY&#39;, &#39;IBM&#39;, 1000, 45.00], [&#39;2006-04-05&#39;, &#39;BUY&#39;, &#39;MSFT&#39;, 1000, 72.00], [&#39;2006-04-06&#39;, &#39;SELL&#39;, &#39;IBM&#39;, 500, 53.00], ] c.executemany(&#39;INSERT INTO stocks VALUES (?,?,?,?,?)&#39;, purchases) conn.commit() c.execute(&#39;SELECT * FROM stocks ORDER BY price&#39;) listbackTable=c.fetchall() pd.DataFrame(listbackTable) . 0 1 2 3 4 . 0 2006-03-28 | BUY | IBM | 1000.0 | 45.0 | . 1 2006-04-06 | SELL | IBM | 500.0 | 53.0 | . 2 2006-04-05 | BUY | MSFT | 1000.0 | 72.0 | . And we can easily beautify the process by making our own mini API functions so that the SQL language will be hidden when reading through the algorithm, and this will also make typing these pesky commands a one-off. . def addTBL(tblName,fields=&quot;&quot;,data=[],addOn=False): &quot;&quot;&quot;Create table if not already existing, optionally with data, optionally clearing out old data if present. Fields as list of strings&quot;&quot;&quot; conn = sqlite3.connect(&#39;example.db&#39;) c = conn.cursor() listedFields=&#39;&#39; for f in fields: listedFields=listedFields+&#39;, &#39;+ f listedFields=&#39;(&#39;+listedFields[2:]+&#39;&#39;&#39;)&#39;&#39;&#39; #Add leading and closing bracket, remove naively added comma,space from leading field c.execute(&#39;&#39;&#39;CREATE TABLE IF NOT EXISTS&#39;&#39;&#39;+tblName+listedFields) # Create table. if addOn==False: c.execute(&#39;&#39;&#39;DELETE FROM &#39;&#39;&#39;+tblName) if data!=[]: c.executemany(&#39;INSERT INTO &#39;+tblName+&#39; VALUES (?,?,?,?,?)&#39;, data) conn.commit() def isNumeric(n): try: n=int(n) return True except ValueError: try: n=float(n) return True except: return False def viewTBL(tblName,fields=None,sortBy=None,filterOn=None): &quot;&quot;&quot;return np array of table with optional select fields, filtered, sorted. Sort syntax=[(field1,asc/desc),(field2,asc/desc)...] Filter syntax=[(field1,value),(field2,value)...]&quot;&quot;&quot; conn = sqlite3.connect(&#39;example.db&#39;) c = conn.cursor() stmnt=&#39;SELECT &#39; if fields!=None: flds=&#39;&#39; for f in fields: flds=flds+&#39;, &#39;+f stmnt=stmnt+flds[2:]+ &#39; FROM &#39; +tblName+&#39; &#39; else: stmnt=stmnt+&#39;* FROM &#39;+tblName+&#39; &#39; #unspecified, select all if filterOn!=None: filt=&#39;WHERE &#39; for f in filterOn: if isNumeric(f[1]): filt=filt+f[0]+&#39; = &#39;+ str(f[1])+&#39; AND &#39; else: filt=filt+f[0]+&#39; = &quot;&#39;+ f[1]+&#39;&quot; AND &#39; filt=filt[:-4] #Remove naively added final &quot; and &quot; stmnt=stmnt+filt if sortBy!=None: srt=&#39;ORDER BY &#39; for s in sortBy: srt=srt+s[0]+&#39; &#39;+s[1]+&#39;, &#39; srt=srt[:-2] stmnt=stmnt+srt stmnt=stmnt+&#39;;&#39; #return stmnt c.execute(stmnt) return np.array(c.fetchall()) . viewTBL(&#39;stocks&#39;) . array([[&#39;2006-03-28&#39;, &#39;BUY&#39;, &#39;IBM&#39;, &#39;1000.0&#39;, &#39;45.0&#39;], [&#39;2006-04-05&#39;, &#39;BUY&#39;, &#39;MSFT&#39;, &#39;1000.0&#39;, &#39;72.0&#39;], [&#39;2006-04-06&#39;, &#39;SELL&#39;, &#39;IBM&#39;, &#39;500.0&#39;, &#39;53.0&#39;]], dtype=&#39;&lt;U32&#39;) . viewTBL(&#39;stocks&#39;,[&#39;symbol&#39;,&#39;price&#39;],[(&#39;price&#39;,&#39;asc&#39;)],[(&#39;symbol&#39;,&#39;IBM&#39;)]) . array([[&#39;IBM&#39;, &#39;45.0&#39;], [&#39;IBM&#39;, &#39;53.0&#39;]], dtype=&#39;&lt;U32&#39;) . viewTBL(&#39;stocks&#39;,[&#39;symbol&#39;,&#39;price&#39;],[(&#39;price&#39;,&#39;asc&#39;)],[(&#39;qty&#39;,1000)]) . array([[&#39;IBM&#39;, &#39;45.0&#39;], [&#39;MSFT&#39;, &#39;72.0&#39;]], dtype=&#39;&lt;U32&#39;) . These custom functions should allow us to very easily perform lookups and filters within the python framework. The key use-case here is that we will sort employees by hours ascending when going in priority sequence of voluntary assignment, but we will sort by seniority descending when retrieving the priority sequence for forcing assignments in vacant slots. Other use cases are filtering the training data to identify what someone is trained on. At the time of writing this, I haven&#39;t yet figured if I will need to make this a truly relational database to make that work. I suspect it wont be necessary as it may be easier to simply perform simple lookups, retrieving key values and then plugging them into where needed using plain python. Time will tell! . ...And a couple of weeks later, I return to this section of the post, battle scarred and weary... I set myself a trap here, and hopefully one I do not soon forget. I cast the ouput of the above functions to np.array() for no particular reason. This brings a reasonable idea to mind- every character in a line of code should ahve a reason for being there. This little maneuver (which I clearly recall came from a sense of satisfaction in being spiffy and referencing a large library) cost me about 8 hours of troubleshooting much later in the process. What really confounded me most was that the issue dind&#39;t present itself until I started using test data sources with entries in all fields. What happened is that a numpy array prefers to be homogenous (storing all members of the same data type) whenever it can: . contents1=1,2,3 contents2=1,2,&#39;3&#39; contents3=1,&#39;2&#39;,None l1=list(contents1) l2=list(contents2) l3=list(contents3) ar1=np.array(contents1) ar2=np.array(contents2) ar3=np.array(contents3) l1,l2,l3,ar1,ar2,ar3 . ([1, 2, 3], [1, 2, &#39;3&#39;], [1, &#39;2&#39;, None], array([1, 2, 3]), array([&#39;1&#39;, &#39;2&#39;, &#39;3&#39;], dtype=&#39;&lt;U11&#39;), array([1, &#39;2&#39;, None], dtype=object)) . as can be seen above, a simple list will just leave its contents be, presenting them as they came in. A numpy array has different behaviour... when a None value is present, other values will remain as they are. However, with even 1 string value, all other numbers within will be converted to strings. And this is evident in the example query output cells. Note the price values have apostrophes around them. I didn&#39;t go so far as to reference those values for further computation when I initially tested these custom functions, and so I had a false sense of safety in using them. And since I had a data field empty throughout my testing, this lured me into a yet stronger and more false sense of security. Finally, I reached the point where I filled in my data, and what broke was the &#39;FilterOn&#39; method of the querying, as I was trying to filter for the employee ID as a numeric value, but it was showing up as a string. And it was a real doozy to trace back because I jumped to assuming the problem was inherent to SQLite3 for Python. What a fool I was. Classic case of troubleshooting best practice: only open so many black boxes as you need to. In this case I jumped to assuming the problem lay deeper in the weeds than it really did, and I wasted a lot of time in those weeds. Only when i went through all the trouble of mocking up little fake databases and entering a few lines did I realize that sqlite itself wasn&#39;t the problem... this one taught me via pain. If you want a list: don&#39;t use a numpy array! . Algorithm Structure . The preceding section is all about just getting the data we need into our hands in a readily manipulable format. Now the fun begins! . The following is the process to be carried out in progressively greater detail.. a fun exercise in breaking a larger problem into sub-problems until a programmable level is reached... . Assign individual to a timeslot | Repeat until all slots filled | That&#39;s accurate but useless to make anything happening... I&#39;ve been thinking about the problem for a while and can list off design questions and answers I&#39;ve reached pre-development. . Preliminary Brainstorm . Which timeslot/job combination should be assigned when multiple are available? Based on this video I learned that to have the best algorithm performance for this kind of problem, you want to perform a depth-first search where you make assignments to the most constrained variables first. From the perspective of assigning timeslots to staff, the sequence in which staff are assigned slots is not optional within the Collective Agreement. With a change in perspective, however, one can see this problem as that of assigning staff to timeslots. From this perspective, a single staff person has a subset of timelsots for which they are eligible to work. Within that subset, this heuristic of assigning to the most constrained slot can be applied. My idea is to maintain a tracking of how many eligible people there are for each job/timeslot combination in the voluntary or forced category, and always assigning people to slots for which they are eligible, for which the fewest people are available. I do have some concerns about whether or not it will work without further consideration as described in the video. Namely, the process of removing assignments from consideration which would leave no assignment available for downstream decisions. I will forgo further worrying about the problem for the time being since I think my problem case is sufficiently different from the one explored in the video. In this situation, it is acceptable and possible that a staff person with priority over another not be assigned, based on their voluntary hours and/or training. | . | How can eligibility criteria for timeslots be made flexible so that the program is useful even after a change to the CBA? My solution to the problem of varied esoteric constraints that are difficult to define, and for which the supplementary data to test constraint criteria is not available, is to ignore these constraints in decision making within the algorithm, instead allowing the user to address these constraints by passing in a lit of enforced assertion statements to enforce some assignments, or disallow other assignments. | My solution to the problem of decision making criteria needing to be transparent and easily modifiable is to have the eligibility criteria for assignments be passed as an input to the system within a file (i.e. python module with functions defined within that will determine assignment eligibility) | . | How can the time slots be represented in if every weekend has different jobs to fill? Again, the idea is to use a file that will be the same most of the time to provide the time slots to be filled as an input to the program | . | How can the algorithm be proofread by a human user to ensure results are valid? It is a distributed process. Each staff person is responsible for communicating if they have been assigned to a slot that they are not eligible for. If the schedulign teams recieves this feedback, or if in schedule review they identify an assignment that seems to be contrary to policy, the program should be able to provide a step by step list of the assignments that were made, as well as being able to visualize a partially done schedule, at an arbitrarily selected step, to give the human scheduler the information required to validate if a generated schedule was valid or not, or if a new assertion should be entered into the input to prevent an invalid assignment from being made. Assertion Types: Do Not Staff: DNS( slot ) | Assign: A( eeid, slot, job, Type) Type can be WWF, Voluntary, or Forced | . | Disallow Assignment: N( eeid, slot, optional job ) | . | . | . | . Back Tracking . While in an ideal world, back tracking could be used so as not to recompute an entire schedule after a new assertion is made, the details of implementation would be very complex, and my initial assumption is that the reduction in computation time and energy would likely not be worth it, so we&#39;ll forgo that for now. . Sneak peek, I&#39;m coming back to this statement days later after programming it and thinking about it a fair deal... my conceptualization now is that back trakcing may in back be entirely necessary to construct the optimal schedule (i.e. one with the least forcing, ehre maximal number of people get voluntary overtime). The version 1 that I&#39;m constructing is essentially a depth first search, without defined decision criteria to facilitate backtracking. The matter of defining how to backtrack is not a simple matter. In a sequence of many nodes, how does one select the origin node to make a different decision at? And if a node is chosen, how does one choose which alternative decision to make, generating the next node? My initial thoughts are that, for the former question, the answer should be that when a given position is identified as requiring forcing, one should look back to the first decision where someone was assigned to something else when they could&#39;ve been assigned to that slot, make that alternative assignment, and then see what bears out using the same algorithm from there. Of course, one issue with this is that the end result may be less optimal and you really woulnd&#39;t know until you compute it all out. Does it reliably get worse, better, or fluctuate between better and worse if this recursive function continues? Or instead of recursing do you return to your original backtrack, and select a different decision node to try a different path from? As one can see, there are many decisions to make when designing a back tracking algorithm for which an optimal decision isn&#39;t obvious at all. For that reason, I&#39;m going to stick to V1 without back tracking for the sake of getting a working prototype up and running instead of spending my whole life on the thought treadmill. If I&#39;m lucky, it works well enough where the human reviewer need only make a couple of changes. The funny thing about this use case is that from the optimization purists perspective this algorithm design is a nasty, horrible mess. From the realists side, good enough may very well be within reach! Time to let go of my purism. . V1 . With these ideas in mind, the algorithm becomes: . Review assertion list and make all prescribed assignments (such as dedicated weekend staff) | Iterate through staff in sequence defined by CBA Apply eligibility constraint functions to timeslots to generate subset of eligible slots. If none, go to next staff person. If one, assign. If multiple, evaluate to see which is most constrained and assign to that. Default constraint functions, applied in sequence of most to least constraining: trained on job | volunteered for slot | minimum 8 hours off between shifts | &lt;60 total hours worked in week On a long weekend, assuming 32 hours worked going into weekend | . | max 12 total hours worked consecutively | | . | . | If slots remain after all staff are iterated through, then: Iterate through all staff from least to most senior, applying modified constraints to determine who must be forced: trained on job | less than 48 hours worked in the week | less than 8 hours forced | minimum 12 hours off between shifts | &lt;60 total hours worked in week | max 12 total hours worked consecutively | | . | . If in the end there is no one available for forcing, then the scheduling team will have to determine whether or not would most likely decide to change which slots are being assigned to move that gap to a different role for which a gap can be sustained in production circumstances. That would be another item to add to the assertion list. . Focusing on Simple . While the above algorithm is what is prescribed by the collective agreement, the rules also state that a given employee has priority selection over another corresponding to which shift they were staffed in the week prior. Carrying out the above algorithm would result in lots of folks being assigned to a shift they volunteered for but don&#39;t have first dibs on; for example the most common preference is day shift but only 1/3 staff were on day shift at a given time. Following the algorithm as initially defined then would lead to significant computational efficiency and the need for more complicated programming to create a cascading/recursive bump management script whereby if someone were assigned to a shift they don&#39;t have priority selection for and it turns out another person has rights to it, the former would be removed, and, since their already being assigned implies they had greater priority to recieve any assignment in general, another slot would have to be sought for them, including slots taken by someone with lesser priority.. and so on. For this reason the actual algorithm is modified to eliminate the need for backtracking (this is analogous to the actual process carried out manually at this time): Instead of iterating through staff in sequence determined by CBA (# of hours), create a separate list of staff, separated by crew and employment type. Carry out the above simple algorithm on each subset of these with the corresponding reduced set of available slots each time. . Summarized: . Carry out above algorithm with on-shift full-timers, for each shift (C/A/B) (in existing data input format, probationaries are included in FT list) | Carry out above algorithm with off-shift full-timers, for each shift (C/A/B) (following the C-&gt;A-&gt;B-&gt;C-&gt;A priority selection format) | Carry out above algorithm with on-shift Temp staff, for each shift (C/A/B) | Carry out above algorithm with off-shift Temp staff, for each shift (C/A/B) | . To conclude, the outer most loop is across the different sets of staff groups, and then the inner loop is across shifts. And once again, this is done so that there should never be a situation where an individual is being bumped out of their slot by another later in the assignment process, simplifying the program implementation overall. . Exploring Problems . A problem that comes to mind. Postulate: the &#39;most constrained first&#39; assignment heuristic, as defined, could generate a schedule structure where someone is passed over for a slot because of the minimum shift gap constraint, when a different valid assignment earlier would ahve created the possibility of a longer contiguous shift, which in fact is what should happen. . Example: Worker A is interested in working an 8 hour shift between 7a and 7p and their priority is daytime (7a-3p). Worker B is interested in working 4 hours between 7a and 3p and their priority is daytime (7a-3p). Higher priority individual B due to the &#39;most-constrained first&#39; assignment heuristic may be assigned the 11a-3p slot, leaving A to be assigned either the 7a-3p, or 3p-7p slot. The problem is that if B were assigned the first slot of the day, the following two slots could both be covered by A, preventing the need to force anyone for the 4 hour gap posed by the former arrangement. . The question is whether or not the postulate/thought experiment bears out... further thoughts follow; in this situation where forcing would be required, that would imply that no one else was available to fill the slots. Looking at the most-constrained heuristic in greater detail, this would mean that when B is assigned, slot 1 and 2 are both tied for 2 potential assignees (A or B), whereas slot 3 is most constrained with 1 potential assignee, A. It seems to me that this is a scenario I could leave under the umbrella of &#39;manual review + assignments&#39; but my gut tells me that is instead a problem of providing decision criteria for when there is a tie in the &#39;most constrained&#39; heuristic between slots. Proceeding with this, it seems obvious to me from the example thought experiment that the criteria should then be a comparison on the number of potential assignees for the slots neighbouring to the one being considered. The challenge in circumventing this problem is in creating a decision criteria where the cure isn&#39;t worse than the disease in terms of code implementation... The central challenge of the entire scheduling problem looms large in this small decision case, which is that the state that assignment variables will take later in the process can&#39;t be known except by carrying out the whole process to get there. The idea that leads me to is to check for a shift-splitting situation as described. That can be done simply by performing the following check: remove the worker whose assignment is being made from the pool of eligible assignees. Observe, then, if there are any sets of 2 or 3 contiguous slots with only 1 and the same worker eligible. Remove those slots from the pool of eligible assignments for the worker whose assignment is being made. Assign to remaining slot pool according to most constrained criteria. If &gt;0 slots are available but insufficient to complete the persons voluntary shift, then return those removed shifts to the eligible pool and connect them. If 0 slots are available when the other were removed, then return them to the eligible pool but assign only slots from the edge of the group. . If one neighbouring slot are unassigned and each have only one and the same potential assignee after removing the assignee in question from the pool. Applied to the same thought experiment, slot one would not be identified as a shift splitting assignment since the previous slot would be assigned. Slot two would be assigned as a shift splitting assignment since slots 1 and 3 each have only A as the eligible assignee after B is assigned to slot 2. Omitting the shift splitting decision from the pool leaves only slot 1 to be assigned to B. Bear in mind that this assumes If both available slots have this criteria, then the fact of which slot comes first can arbitrarily be used to break the tie... This is because when it comes down to brass tacks, that individual B is entitled to their OT selection in that scenario even if it forces that shift split and leads to someone being forced, or a gap. The problem of a gap can be addressed outside the context of the program. . The former problem posed by someone selecting a small 4 hour block also brings to mind the other problem of people volunteering for 12 hour blocks. The challenge is that the algorithm is defined as looking first at each shift (8 hour blocks), but staff are eligible for 12 hour blocks across shifts, or 8 hour blocks straddling shifts. . I&#39;m at the point in this thought experiment now where I think that trying to implement a problem specific solution here has too great a risk of introducing unintended consequences that bring failure. For the sake of time and simplicity I&#39;ll proceed with a version 1 that leaves the resolution of these issues in the hands of the user via the forced assignments function. . Data Structure . To make the program easy to maintain, debug, and code, the data structure of classes/objects/attributes and their relationships should be carefully constructed to facilitate the intended actions. My goal is to have a rigorously modular/generalized system, where almost every process in the final algorithm is a method on an object. This will make the code readable, flexible, and easier to debug in development and deployment. With the above algorithm in mind, I made a dummy script to let inuition guide the insight as to necessity of what classes/attributes/methods would be required: . CollectData(): pullTables() # Per code in above sections configData() # Define timeslots objects, worker objects, collections of workers per shift/type Schd=Sched(date) Schd.preFill() # Iterate through &amp; enact prescribed assignments numLoop=0 while Schd.openSlots.count&gt;0: #Because forcing can open someone up for voluntary, need to make this looping capacity. Tracker bit prevents inf loop numLoop+=1 Schd.VolunteerFill(): for eeTypePool in (onShiftFT,offShiftFT,onShiftTemp,offShiftTemp): for shift in shiftSet: #shiftSet is built based on what days selected to schedule. Always seq last to first. namePool=poolPicker(shift,eeTypePool) for person in namePool: #Idea: Define a generator function to yield the next person, across ee categories slotPool=filterSlots: #per sequence above, evaluate each criteria in sequence and remove slot from pool if fails any criteria isTrained, Volunteered, shiftGapOK, wklyTotOK, maxShiftLenOK if poolEmpty: next person s=pickSlot #Most constrained slot (if *only* person avail for off-shift, assign there. If only person for multiple, assign first chronological. If candidates&gt;1, take most constrained on-shift slot. If tied, take first chronological) assignSlot(s,type=voluntary) #Perform necessary functions i.e. removing tally of op from no-longer compatible shifts. Schd.forceFill(): for slot in Sched.unassigned.chronologicalSeq: assignee=lowMan(slot): filter all ee for training, sort seniority low-hi, check if already worked 8+ hours, check if already forced 8 hours #if no assignee, flag slot as &#39;no staff&#39; result=unassignAsNecessary(assignee,slot) check if the forcing would conflict with other constraint (inter-shift gap, weekly total hrs, shiftduration, etc) if the conflicting slot requiring unassignment is from the assertion list, then return an error flag for printout assignSlot(slot,type=forced) if numLoop&gt;5: break . Optimal vs Proven? . The algorithm explored above is my attempt at making explicit the process carried out by the human schedulers at present. This method hinges on scheduling one shift at a time with the priority selection staff for that shift, then proceeding to last with less priority. The thing is, I know I want my first version not to support solution tree exploration (backtracking and testing of different assignments). And trying to emulate that human method is sticky as hell because it really does demand that each stage, a check be made so that, even if someone might&#39;ve had indicated willingness for a slot they had priority for, they need to be assigned to a slot if they are the only willing assignee for it.. Incorporating this pre-check just feels so janky to me in the scheme as it is presented above. So I&#39;m thinking that that&#39;s because it&#39;s wrong to go about it. I&#39;m going to make the assignment algorithm assign opposite from how schedulers do it, by assigning staff to slots in the sequence of which slot is most constrained (respecting staff priority assignment sequence), as opposed to assigning slots to staff in the sequence of who has priority pick. And I think this will work just fine because the means by which staff will be selected for assignment to a slot will follow their priority sequence. As postulated previously, it will take implementing and testing to prove that this will work and in fact be better than the alternative w.r.t minimizing overall forcing. But I wouldn&#39;t be surprised if, in the grand scheme, this leads to scheudles that are qualitatively different from ones human might make (for example, having a single person scheduled on different jobs across their shift when they could&#39;ve stayed on the same one because they were interchangeable with the person on the other slot.) . Adjustment Required . I got a draft algorithm running smoothly but then hit a hitch when it came to logic... . My process was as follows.. . Start by forcing to all slots with no volunteers | Then fill all slots via voluntary takers | Finally, force again for all slots that might&#39;ve been left open | . This worked but the problem was as follows... individuals can only be forced to a maximum number of hours worked in the week. This meant that, although an individual might be in line to be forced on a weekend shift, if they elected to work shifts taking place earlier in the same weekend, that can put them at their weekly limit and make them not eligible for forcing. In this way, it becomes necessary to modify the process to account for this, so as not to have someone forced into a shift in Phase 1 when they volunteered theirselves out of it but that was only determined in Phase 2. . My first idea was to sweep through the schedule, assigning to all jobs in chronological order, one timeslot a time. The issue with this is that if I reduce the scope of &#39;most constrained slot&#39; to a single concurrent slot, then this opens up the problem again of assigning someone to a slot which has volunteers to spare when that same person is the only volunteer for a different slot and so should cerainly be assigned there. My idea to resolve this is as follows: . First, evaluate which slots have only 1 eligible volunteer, and mark those people as such. | Second, sweep through the schedule, but instead of considering only a single time slot at one time, consider all time slots occurring prior to the first slot requiring a forcing for lack of volunteers. Assign all these slots to volunteers in sequence of most constrained If making one such voluntary assignment and removing the individual from eligible volunteers to another slot leaves another slot with only one eligible volunteer, mark that person | When a marked person being assigned to some voluntary slot according to the usual preferential selection sequence would result in them being unavailable for a slot they were marked for, then assign the next person in sequence, saving that individual for the marked slot | . | Evaluate forced slots as they are reached in chronological order of shift time | . Unfortunately this introduces extra challenges but I feel it is necessary to eliminate the possibility of the algorithm forcing someone into a slot that they shouldn&#39;t be forced into... more functions will have to be made to evaluate some of these unique circumstances which must now be considered. Necessity is the mother of invention. . Adjusting Adjustments - Sept 12 /22 . A different possibility occurred to me: proceed from the get go assigning slots, sequenced by most constrained. This means starting off my forcing dslots without volunteers, in chronological order. At each assignment, properly update all slots (decrement eligible volunteers) who can no longer be assigned the worker as a result of the new constraint made the by the assignment just entered. Here is the caveat: when a slot is decremented to 0 eligible volunteers, consider this fact that the assignment strategy results in a forcing - re make the schedule from scratch, except this next time around, add that slot that needed forcing to the initial forcing phase. . I think this would work better than the previous idea... The reason is that, at every step of the way, the most constrained slot is still being assigned, avoiding the issue that could be faced if moving through chronoglogically with ones horizon limited by force slots, which could result in somone being assigned to a populated slot when they were the only taker somewhere else later on. . This strategy would bypass the issue of &quot;When I go back in my search tree, to which node to I go, and how do I select an alternative edge to search from that node?&quot; by flipping the paradigm. It demands scrapping the old exploration tree, in favour of a new seed, one generated with the knowledge that proceeding with assignments per most constrained criteria would result in a forced slot... by forcing that slot in the beginning this time around, you ensure you haven&#39;t made the problem of assigning that person voluntary overtime later when they wouldn&#39;t be eligible for it due to being forced earlier. Arguably more, arguably less computation work to start from scratch, but either way, ensures the problem of &#39;how to backtrack&#39; is avoided. I do think I will try to implement this for sure as it should also be much more easy to code, I think. Aside from copying the seed before starting and applying a loop, all I need to do is to check all slots someone is trained on against the already defined slot eligiblity checker function to facilitate the continuous updating/removing of an individual from the tally of eligible volunteers for a given slot. . There is a loose string here, though. What is the decision criteria not to loop back to the original seed? I mean, the need to force is encountered, so we return to square one, and force from the get go. Assuming we have someone to force in, then that slot does not appear for forcing again. But another slot appears. So we start from scratch again. When does this end? The logical answer to me seems that a log is kept of which slots were encountered in this way, and that the process only start from scratch when its the first time a given slot is finding itself in need of a forcing as a result of voluntary assignments taking away the last voluntary overtime taker. The search down the exploration tree should go deeper and deeper each time around until finally all slots are assigned (or fail to be assigned via forcing for lack of staff). And this means that all forcings would be made first, before any volunteer scheduling. But this poses a problem again that I forgot about in my rush on this... how then to know if someone can be forced if you haven&#39;t yet determined if they elected to work prior to that shift? Again I feel bamboozled by this issue of the not being able to foresee the future of an algorithm. I worry I may be fighting against an insurmountable wall here, like the Game Of Life in a different form... For now I think I will set this problem aside and proceeed with this idea as is. usually, very few people get forced and so by shoving the potential algorithm error into the issue of &#39;they could&#39;ve volunteered prior to their forcing&#39;, in fact I believe it makes it easier to evaluate for algorithmic error. Instead of an error being who knows where and more difficult to find, I think this method would make it so that, if a person was forced and was indeed meant to be forced, then you can be sure there are no more errors, since from there on the schedule was all built by assigning the most constrained slots in sequence. And if it was an erroneous forcing in the sense that the person should&#39;ve gotten voluntary time prior to it, then that can be added to the assignment list and the program re run. . I had plenty of time to think of this while making the first version of the fractional shift indicator code. The part that adds 1/2, or 1/3, 2/3 etc beside peoples names on their slots to help identify full shifts. That was a much bigger pain than I thought it would be, unfortunately. And just when I thought I had it working great, I test a different use case and its completely knackered... big sigh. . Developments Sept 17 /22 . I got the 1/2,2/3,3/3 script working. It wasn&#39;t too hard in the end. What took the past few days was trying to implement the recursive scheduling strategy. I&#39;ve got it working now (mostly?) though not without difficulties along the way with typos, logic errors, and accidental infinite loops. Par for the course, of course. But what a pain! I am now seeing what I hope is truly the final problem... the triggers I defined as reason to start a new schedule from the top are as follows: when assigning a slot, you follow up to see what slots that person could previously do which now they can&#39;t due to being assigned to the slot at hand. If one of those other slots now has no volunteers, thats the trigger to add that slot to the forcing list, and restart the scheduling loop. But I realized when making these things that it isn&#39;t right to think of it as the forcing phase. Rather, it is simply the priority phase. That is, before any assignments are made, we think we have the priority sequence of slots to assign based on volunteers available, but the process of assigning changes that sequence, as we find with our checking. So really phase one is not a forcing phase, it is simply the phase where we assign the slots we know to be most constrained, and that info only comes from previous iterations where the restart was triggered. So... the trigger condition comes from checking which slots are no longer eligible for. But the problem I see now is the problem of forcing limitations. Specifically, when I was doing the check on &#39;What other slots are they still an eligible volunteer for?&#39;, I was making that check assuming all the OT was voluntary... but some slots may or may become ineligible depending on how many hours they&#39;ve been forced thus far in the week and how many have been worked total in the week... The failure mode that revealed this to me was that after the priority forcing phase, a person was scheduled for voluntary time prior to their 8 forced hours. This created a situation where the person was bein forced after 48 hours in the week, which is not allowed. As I see it, the solution cna be approached from one side or another: trying to fit this check in for all potential slots that someone could be assigned to, when they are assigned to another slot, or simply checking if the rule is being broken after assinging someone a slot. I think the latter method is easier (iterate through assigned slots and run the total hours/forced hours etc.) but the latter identifies the issue before assignment, which is aligned with the other methods I&#39;m employing. Nowhere else is the condition checked after assignment. I think doing it before assignment makes more sense from a strategy and consistency perspective. So the form that takes is as follows: when a person is scheduled to a slot, observe if they are forced for any hours, and if so, remove their eligiblity for slots before forcing such that the forced hours cannot go past 48 in the week... in this way, if that person was the only eligible volunteer for a slot prior to their forcing, that slot will then be identified as having no volunteers, putting it in the priority assignment phase, where it will be forced (or voluntary assigned) prior to the other one which was being assigned. I remember in my previous update I mentioned the methodology I was using would leave the possibility open of not assigning people to slots prior to their forcing.. I don&#39;t know if thats still possible. My brain hurts. I can&#39;t tell if I&#39;m eliminating the failure modes entirely, or just one type of failure mode (&gt;48hrs voluntary before forcing). But I think its good.) . Well then I thought to myself. If I remove their ID from the list of elibile volunteers for a slot, but this forcing past 48 thing isn&#39;t actually evaluated for in the function that checks if a slot is ok to assign, doesnt that leave the failure mode open still? Yea... in a situation where there are other eligible volunteers for that slot, the trigger to restart wouldn&#39;t be hit.. and if the person whos ID was removed from the list had priority assignment to that slot before the other people, then they would be selected and assign, though still breaking the rule. And I can&#39;t make their id being present in the volunteer list as a &#39;slot OK&#39; check criteria because I already have a failsafe situation where someone might be an eligible volunteer though their name isn&#39;t in the list. So I can&#39;t just make the opposite happen here... I will have to resort to after-assignment checking of the rule being broken I suppose. . Success Sort of - Sept 19 . It really seems like its fully working, but latest real world weekend schedule had a lot of particularities that were unusual which would&#39;ve taken me a long while to input, so I didn&#39;t proof check it with that real data. We&#39;ll give it a go next week! . If I can make the HFS side of it work in time. Doing that today. . Progress, Surely - Sept 26 . I got that working later that same night. The next week, I trialled it using the previous weeks weekend data but... I was sunk by the edge cases of it. That week was a real gong show of peculiarities in the schedule. Trying again this week and the results are promising but not perfect. I&#39;m seeing people being forced in for 12 hours, people forced who are on long term leave, and it failed to assign someone for 12 even though they were willing, forcing in lieu. The long term leave thing is a simple fix, but the forcing and failure to assign 12 hours is a bit of a mystery . Could This Be It? - Sept 27 . Fixing people on long term leave was easy but still a nice classic little exercise in a quick and dirty way appearing to work but not really, or an equally quick way that really worked. The former I tried first was to not define an employee if their crew wasn&#39;t one of the accepted values. Rapidly hit some snags such as a variety of unexpected values, and errors being thrown when the ee object wasn&#39;t created for someone who was on WWF list who I failed to mark as inactive due to vacation. The better way was to add a new check on the &#39;slotOK&#39; check function to see if the persons crew is valid. This way all ee objects are still made so there aren&#39;t any errors thrown when trying to reference one that will ultimately be invalid. . The matter of people being forced in for 12 hours was simply because I had forgotten to select &#39;40 hour week&#39; instead of &#39;32&#39; and so people were forcable for up to 16 hours to take them to 48 total in the week. The &#39;failing to assign, forcing in lieu&#39; was a misread on my part as there were valid rules being followed that prevented that eprson from being eligible though they were volunteering. . I encountered some other interesting issues when troubleshooting such as identifying a situation where someone was absent for multiple days in the weekk, but did OT on the other days. Since the sheet isn&#39;t given sick hours, it made it seem that the person wasn&#39;t forcable (past 48 hours) but really they weren&#39;t. Until/unless that capability gets built in, forcings would always have to be double checked. . A more interesting problem was that when I put a specified assignment in the assignment list, it was being implemented but at the same time, not... the persons other slot indicated 2/2, implying the other job was there, and the slot was present in the employee object assignment list, but the slot was assigned someone else. It turns out what was happening here was that when the list of slots to assign to is initially defined, it only omitted slots with &#39;DNS&#39; or &#39;WWF&#39; assignment, when it shoukld&#39;ve included &#39;V&#39; and &#39;F&#39; as well for those cases where those assignments were pre assigned in the assignment list. Since that was missing, the assignments were made but then the slots were re assigned (overwriting slot info but not original assignee info) resulting in the bug. . ..... . And.. I&#39;ve reviewed the real life version and the automatic version, and that&#39;s it! They aren&#39;t a perfect match, but for every different assignment, the automatic version is what it should be based strictly on the input data. The remaining differences I put down to human judgement or people changing their minds about working after the fact not being reflected in the polling sheet. Success! . Adjustments Required - Sep 29 2022 . After review with the team, identified issue that it needs to give precedence to someone who wants 8 hours, even if they are lower in the selection sequence than someone who wants 4 hours. The problem I stopped at when trying to do this in earlier versions was that it became very complicated when choosing to explore neighbours to the left or right (slots chronologically before or after). Its further complicated by the issue of shift-selection precedence and the possiblity of someone taking an 8 hour shift straddling two shift periods, meaning the sequence of priority selection is different across those slots. The solution we came to is a slight injury to my pride, but I can live with that. That&#39;s because it is in fact a return to what I originally said could not be, which is assigning the slots the way the people working the process do it currently. Which is to tierate through the 3 shifts of the day, and iterate through the priority selection sequence assigning people the slots they want. But first to do that only for people who want full 8 hour slots on the shift they were scheduled for this past week. What this does is essentially take away one part of that &#39;which side to explore&#39; question. By immediately assigning 8 hour shifts within the defined night, day, and afternoon shift periods to anyone who is eligible (chosen in priority sequence), then slots that are unassigned going into the next phase can only become a aprt of an 8 hour shift if they become one half of a straddled shift. That makes it only a one way search, much easier. So this can be iterated upon to identify all straddled 8 hour shifts, or necessarily any 12 hour shift. Finally, any slot that remains unassigned is open to 4 hour assignments. Time to implement. . -- Few hours later. Good progress. I&#39; am for once implementing the ebst practice of testing that its working after every couple of lines that I write. hopefully this will prevent need for any debugging once last line is written. Have already added the differentiation between Probationary,full time, modified assignee selector to reflect what that sequence should be, and a few other things. So tempted to continue but need to go to the gym. . Painful - Sep 29 2022 . Many hours today. Finally lots of progress making it prioritize 8 hour blocks over 4, but in the home stretch right now and I can&#39;t manage tot ie the bow on it... one last nagging issue here of leaving one single slot open when it sohuld be forced and I cant understand why its overlooking that. . Bloody bloody bloody lessons learned - caution with using sub functions inside recursive function. Caution with version tracking. experimenting with different changes in different files. Caution especially with that latter item, and importing those modules within other modules (changed file anme and reflected in main but not in the other auxiliary also referencing that file) and AGAIN with copying and pasting old code and assuming it would work (nto recursing when final phase (4hr assn) caused slot with no volunteers) and also again having the right logic but implementing incorrectly (&quot;phase 0.5&quot;... no... phase 0 had to go through with pre8 in chronological order and assign pre8 if observed.) And also keepign in mind that append() retunrs but extend() does NOT. what a waste . Thought I had it going but... seems not.. when using only can lines, seeing wierd things. Seems like some WWF spots being over written, among others things as well.... see &#39;cans only&#39; file . also the other one (&#39;rev 1&#39;) kicked out after 25 iterations with more spaces to go but can also see things wrong there.. check it out... trace it back.. .",
            "url": "https://davidd003.github.io/Posts/jupyter/2022/08/11/Deploying-A-Schedule-Building-Algorithm.html",
            "relUrl": "/jupyter/2022/08/11/Deploying-A-Schedule-Building-Algorithm.html",
            "date": " • Aug 11, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Building A Digit Classifier",
            "content": "from sklearn.datasets import load_digits mnist=load_digits() . mnist.keys() . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;feature_names&#39;, &#39;target_names&#39;, &#39;images&#39;, &#39;DESCR&#39;]) . mnist[&#39;target&#39;] #Observing y value for data sequence . array([0, 1, 2, ..., 8, 9, 8]) . [list(mnist[&#39;target&#39;]).count(i) for i in range(10)] #Count of each digit in dataset . [178, 182, 177, 183, 181, 182, 181, 179, 174, 180] . len(mnist[&#39;target&#39;]) #Size of dataset . 1797 . mnist[&#39;images&#39;][-1]/16 . array([[0. , 0. , 0.625 , 0.875 , 0.5 , 0.0625, 0. , 0. ], [0. , 0.125 , 1. , 0.875 , 0.375 , 0.0625, 0. , 0. ], [0. , 0. , 0.9375, 0.9375, 0.5 , 0.9375, 0. , 0. ], [0. , 0. , 0.3125, 1. , 1. , 0.625 , 0. , 0. ], [0. , 0. , 0.75 , 0.9375, 0.9375, 0.75 , 0. , 0. ], [0. , 0.25 , 1. , 0.375 , 0.25 , 1. , 0.375 , 0. ], [0. , 0.5 , 1. , 0.625 , 0.5 , 1. , 0.5 , 0. ], [0. , 0.0625, 0.5 , 0.75 , 0.875 , 0.75 , 0.0625, 0. ]]) . show_image(mnist[&#39;images&#39;][-1]/16) #Visualizing example digit . &lt;AxesSubplot:&gt; . stacked=[] for i in range(10): #This loop because stacked=[[]]*10 makes 1 list in list, with 10 copies of pointers... need separate objects stacked.append([]) for i in range(len(mnist[&#39;target&#39;])): #Assign all images to the right collection in the &#39;stacked&#39; list, indexed by target stacked[mnist[&#39;target&#39;][i]].append(mnist[&#39;images&#39;][i]) . lens=[len(stacked[i]) for i in range(10)] lens, min(lens) #Confirm counts of samples . ([178, 182, 177, 183, 181, 182, 181, 179, 174, 180], 174) . stacked=tensor([x[:174] for x in stacked]) . /opt/conda/lib/python3.7/site-packages/fastai/torch_core.py:151: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1634272168290/work/torch/csrc/utils/tensor_new.cpp:201.) else torch.tensor(x, **kwargs) if isinstance(x, (tuple,list)) . test=[dig[-20:] for dig in stacked] train=[dig[:-20] for dig in stacked] . [len(test[i]) for i in range(10)],[len(train[i]) for i in range(10)] #Confirm counts of samples #Confirm counts of samples . ([20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [154, 154, 154, 154, 154, 154, 154, 154, 154, 154]) . show_image(stacked[3][0]) #Check sample . &lt;AxesSubplot:&gt; . type(stacked),type(stacked[0]),type([stacked[0][0]]) #Check types . (torch.Tensor, torch.Tensor, list) . type(train),type(train[0]),type(train[0][0]),[type(test),type(test[0]),type(test[0][0])] . (list, torch.Tensor, torch.Tensor, [list, torch.Tensor, torch.Tensor]) . train=torch.stack(train) #Converts PyList of tensors to tensor of tesnors (join in new dimensions, retain target indexing) test=torch.stack(test) . train.shape,test.shape . (torch.Size([10, 154, 8, 8]), torch.Size([10, 20, 8, 8])) . stacked[0].ndim,stacked[0].shape #Check dimensionality . (3, torch.Size([174, 8, 8])) . means=torch.stack([x.mean(0) for x in train]) #Compute the average digit for i in range(10): show_image(means[i]) . F.l1_loss(test[0][0],means[0]),F.mse_loss(test[0][0],means[0]).sqrt() . (tensor(1.4324), tensor(2.2704)) . def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) mnist_distance(train,means) # Intentional error to demonstrate . RuntimeError Traceback (most recent call last) /tmp/ipykernel_48/3541699495.py in &lt;module&gt; 1 def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) -&gt; 2 mnist_distance(train,means) # Intentional error to demonstrate /tmp/ipykernel_48/3541699495.py in mnist_distance(a, b) -&gt; 1 def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) 2 mnist_distance(train,means) # Intentional error to demonstrate RuntimeError: The size of tensor a (154) must match the size of tensor b (10) at non-singleton dimension 1 . means.shape, train.shape,means.unsqueeze(1).shape #Failure to broadcast generates above error because broadcasting requires equal dimension #with different size. Unsqueeze to add dimension . (torch.Size([10, 8, 8]), torch.Size([10, 154, 8, 8]), torch.Size([10, 1, 8, 8])) . res=mnist_distance(train,means.unsqueeze(1)) print(res,res.shape) #The result is 10 vectors each with 154 elements. Per the shapes spit out by previous cell #you can see that the Means is being broadcast across the singleton dimension. What this means #is that vector n in the 1st dimension of the result contains the result of the mean value for digit n compared #against all 154 samples of train data for digit n. #This allows us to see the best and worst samples of each digit #View &#39;best&#39; and &#39;worst&#39; digits bestWorstIndex=[(list(x).index(min(x)),list(x).index(max(x))) for x in res] i=0 for b,w in bestWorstIndex: show_image(train[i][b]) show_image(train[i][w]) i=i+1 . tensor([[1.0852, 1.7360, 1.5692, ..., 1.4915, 1.1106, 1.2622], [1.7138, 2.3743, 2.4055, ..., 2.9948, 3.0561, 3.4396], [3.0636, 2.3867, 1.5756, ..., 1.7931, 2.0594, 1.7491], ..., [2.2926, 2.0388, 2.7857, ..., 2.2317, 2.0384, 2.2423], [2.1914, 2.4976, 2.1855, ..., 2.0434, 2.7443, 3.2455], [2.4969, 2.7304, 2.5003, ..., 1.8816, 2.7387, 2.2446]]) torch.Size([10, 154]) . #% accuracy for our baseline model. In that case we want each digit in test set compared #against every mean digit, and then to observe, for each test digit, comparison against #which mean yielded least difference? To do that we need to broadcast more print(&#39;shape of means :&#39;+ str(means.unsqueeze(1).unsqueeze(1).shape)) print(&#39;shape of train data :&#39;+ str(train.unsqueeze(0).shape)) #First of all, we want to unsqueeze the training tensor so that we can broadcast it up, creating multiple copies of itself #in that higher dimension. We want to have one copy of the training data for each of the mean (&#39;ideal&#39;) digits #for comparison, thats why we do that. Creating singleton dimensions allows for broadcasting to essentially create copies of #the data contained in the lower dimensions of the container. # #The means s unsqueezed multiple times because we want copies of copies of each of the mean digits. We unsqueeze at the first #index so that the dimension where the differentiation between digits occurs stays at the highest dimension # #The result is that the 10,1,1,8,8 tensor and 1,10,154,8,8 tensors are broadcast to be equal in shape to perform #computation. First in dimension 1, the train data is broadcast (10 copies created), in dimension 2 the mean data is broadcast #creating 10 copies of everything below. Then in the 3rd dimension, the means is again broadcast up to 154, creating 154 more #copies of what is in the dimensions below. In this way, the 1st dimension corresponds to the different &#39;ideal&#39; or mean #digits 0-9, the 2nd dimension corresponds to all the data corresponding to the training data for digits 0-9. The 3rd #dimension differentiates between individual samples of a given training digit. And the 4th and 5th dimension get us to #individual pixels # all_comparison=mnist_distance(means.unsqueeze(1).unsqueeze(1),train.unsqueeze(0)) print(&#39;shape of all comparison: &#39;+str(all_comparison.shape)) #The result is 3 instead of 5 dimensions because the mnist function took the average across the last two dimensions #reducing the data in them to a scalar stored in the 3rd dimension. So for axis i in the first dimension, we have #10 collections of data, which is the ideal digit i compared against the 154 samples for each digit as indexed in #the 2nd dimension #Picturing the 3D result as a cube, each element in the cube contains the numeric result from mnist_dist for the comparison #of ideal and test image. For a given test digit, it is compared against all 9 ideal digits, and the miniumum mnist_distance #should correspond to the digit that the training digit actually is. Therefore we iterate through taking slices of the cube #where in each iteration, a slice corresponds to all 154 training images for one and the same digit, each compared against #all 9 ideal digits def acc_rslt(comp): c=comp.clone() x,y,z=[i for i in c.shape] totSamp=y*z totCorrect=0 #Tallier confM=[] #confusion matrix will be result of stacking the bincount results for i in range(10): #Taking slice yields 2D object, shape (10,154), take min in each column (axis 1) to get digit prediction id=c[:,i,:].min(dim=0).indices # Retrieve indices i.e. predictions for all comparisons predCnt=torch.bincount(id,minlength=10) #Yields a 1D tensor with count of integers indexed by integer totCorrect=totCorrect+predCnt[i] confM.append(predCnt) confM=torch.stack(confM) return (totCorrect/totSamp*100),confM acc,conf=acc_rslt(all_comparison) print(&#39;accuracy: &#39;+str(round(acc.item(),2))+&#39;%&#39;) df = pd.DataFrame(conf) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) df.style.set_caption(&quot;Top Axis: Predicted value. Left Axis: Actual Value&quot;) . shape of means :torch.Size([10, 1, 1, 8, 8]) shape of train data :torch.Size([1, 10, 154, 8, 8]) shape of all comparison: torch.Size([10, 10, 154]) accuracy: 89.94% . Top Axis: Predicted value. Left Axis: Actual Value &nbsp; 0 1 2 3 4 5 6 7 8 9 . 0 153 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 124 | 8 | 1 | 0 | 2 | 4 | 0 | 4 | 11 | . 2 0 | 6 | 136 | 4 | 0 | 0 | 0 | 2 | 5 | 1 | . 3 1 | 0 | 1 | 143 | 0 | 0 | 0 | 4 | 2 | 3 | . 4 1 | 3 | 0 | 0 | 143 | 0 | 0 | 7 | 0 | 0 | . 5 1 | 0 | 0 | 1 | 1 | 131 | 3 | 0 | 0 | 17 | . 6 1 | 1 | 0 | 0 | 1 | 0 | 151 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 1 | 1 | 0 | 152 | 0 | 0 | . 8 0 | 15 | 2 | 2 | 0 | 4 | 2 | 2 | 119 | 8 | . 9 0 | 3 | 0 | 5 | 4 | 1 | 0 | 6 | 2 | 133 | . train_x = torch.cat([x for x in train]).view(-1, 8*8) test_x = torch.cat([x for x in test]).view(-1, 8*8) train_y, test_y =[],[] for n in range(10): train_y.extend([n]*154) test_y.extend([n]*20) train_y=tensor(train_y).unsqueeze(-1) test_y=tensor(test_y).unsqueeze(-1) train_x.shape,test_x.shape,train_y.shape,test_y.shape . (torch.Size([1540, 64]), torch.Size([200, 64]), torch.Size([1540, 1]), torch.Size([200, 1])) . dset=list(zip(train_x,train_y)) #Zip each input data item to its target output valid_dset=list(zip(test_x,test_y)) #Define DataLoader objects to pass to learner dl=DataLoader(dset,batch_size=25,shuffle=true) valid_dl=DataLoader(valid_dset,batch_size=5,shuffle=true) dls=DataLoaders(dl,valid_dl) . Implementing Net, Learning using custom defined model, learner.. . def init_params(size,std=.1): return (torch.randn(size)*std).requires_grad_() . torch.randn?? . w1=init_params((8*8,1)) #Have to provide extra dimension for matrix mutiplication, making it a single-column vector b1=init_params(1) w2=init_params((1,10)) # Have to provide an extra dimension (1) for matrix multiplication b2=init_params(1) #w1,b1,w2,b2 . def myModel(xb): res= xb@w1+b1 res=res.max(tensor(0.)) res=res@w2+b2#returns 10 features for each input return res . def myModel(xb): res= xb@w1+b1 #res=res.max(tensor(0.)) res=res@w2+b2#returns 10 features for each input return res . mini_samp=5 mini_x=train_x[:mini_samp] #Define a minibatch to test that the model works as intended w.r.t broadcasting across different tensor sizes... mini_y=train_y[:mini_samp] . myModel(mini_x) #test model.. output is a tensor of 10 elements per input image.. #each to be interpreted as activation for a given digit as being predicted . tensor([[ 3.3562, -3.1369, -3.0656, -2.9660, 3.1947, 2.4069, 3.5504, -3.1804, -2.6910, -2.5367], [ 4.2326, -4.0526, -3.9617, -3.8346, 4.0265, 3.0212, 4.4803, -4.1081, -3.4837, -3.2867], [ 3.1284, -2.8989, -2.8327, -2.7402, 2.9784, 2.2472, 3.3086, -2.9392, -2.4849, -2.3417], [ 5.2939, -5.1616, -5.0468, -4.8864, 5.0338, 3.7653, 5.6065, -5.2316, -4.4436, -4.1951], [ 4.4871, -4.3187, -4.2219, -4.0869, 4.2681, 3.1997, 4.7504, -4.3776, -3.7139, -3.5046]], grad_fn=&lt;AddBackward0&gt;) . def my_loss(preds,target,lr=0.01): #softmax returns a larger value for a given index, the larger that number was in proportion to the others #it motivates the network to reduce confidence in other choices at same time as increasing confidence in single output #Take the inverse since the goal is to reduce loss #Understanding that the largest activation is the prediction. # Recall the input is variable [minibatch] size. maxes=F.softmax(preds,dim=-1) correctIndices=[r[t] for r,t in zip(maxes,target)] resy=[-torch.log(tens) for tens in correctIndices] return sum(resy) #The above lines were tested to ensure the operations (softmax, indexing, -log, sum) retain gradient . maxes=F.softmax(myModel(mini_x),dim=-1) torch.log(maxes) #Observe! The softmax loss function yields -inf sometimes which will not allow for computaiton of meaningful gradient.. #If it didn&#39;t this time, just try different input (re run parameter initialization) . tensor([[ -9.9796, -0.6258, -1.2575, -6.4255, -4.6406, -3.7542, -10.6129, -8.0086, -4.6604, -1.9932], [ -6.6498, -0.9041, -1.2921, -4.4667, -3.3703, -2.8258, -7.0388, -5.4391, -3.3824, -1.7440], [ -8.9221, -0.6952, -1.2507, -5.7962, -4.2263, -3.4467, -9.4791, -7.1886, -4.2437, -1.8978], [ -1.0974, -9.4461, -8.8823, -4.2696, -5.8627, -6.6539, -0.5322, -2.8566, -5.8451, -8.2257], [ -4.8328, -1.1909, -1.4368, -3.4490, -2.7541, -2.4089, -5.0793, -4.0654, -2.7617, -1.7233]], grad_fn=&lt;LogBackward0&gt;) . #to numeric underflow, resulting in trying to take the log of 0, and having undefined (-inf) results #that cannot be differentiated.. maxes=F.softmax(myModel(mini_x),dim=-1) correctIndices=tensor([r[t] for r,t in zip(maxes,mini_y)]) myModel(mini_x),mini_y,maxes,correctIndices,torch.log(correctIndices) . (tensor([[-4.1185, 5.2352, 4.6036, -0.5644, 1.2204, 2.1069, -4.7518, -2.1475, 1.2007, 3.8679], [-2.4601, 3.2856, 2.8976, -0.2769, 0.8195, 1.3640, -2.8491, -1.2493, 0.8073, 2.4457], [-3.6006, 4.6264, 4.0708, -0.4746, 1.0952, 1.8748, -4.1576, -1.8670, 1.0779, 3.4238], [ 4.0184, -4.3303, -3.7665, 0.8462, -0.7469, -1.5381, 4.5836, 2.2592, -0.7293, -3.1099], [-1.4931, 2.1488, 1.9029, -0.1093, 0.5857, 0.9308, -1.7396, -0.7257, 0.5780, 1.6164]], grad_fn=&lt;AddBackward0&gt;), tensor([[0], [0], [0], [0], [0]]), tensor([[4.6336e-05, 5.3481e-01, 2.8437e-01, 1.6197e-03, 9.6515e-03, 2.3419e-02, 2.4597e-05, 3.3259e-04, 9.4631e-03, 1.3626e-01], [1.2943e-03, 4.0489e-01, 2.7469e-01, 1.1486e-02, 3.4380e-02, 5.9262e-02, 8.7718e-04, 4.3435e-03, 3.3966e-02, 1.7481e-01], [1.3341e-04, 4.9899e-01, 2.8630e-01, 3.0391e-03, 1.4606e-02, 3.1851e-02, 7.6432e-05, 7.5518e-04, 1.4355e-02, 1.4990e-01], [3.3373e-01, 7.8998e-05, 1.3882e-04, 1.3987e-02, 2.8436e-03, 1.2890e-03, 5.8731e-01, 5.7462e-02, 2.8941e-03, 2.6769e-04], [7.9645e-03, 3.0395e-01, 2.3768e-01, 3.1778e-02, 6.3669e-02, 8.9912e-02, 6.2240e-03, 1.7157e-02, 6.3182e-02, 1.7848e-01]], grad_fn=&lt;SoftmaxBackward0&gt;), tensor([4.6336e-05, 1.2943e-03, 1.3341e-04, 3.3373e-01, 7.9645e-03]), tensor([-9.9796, -6.6498, -8.9221, -1.0974, -4.8328])) . lossResults=my_loss(myModel(mini_x),mini_y) #Testing the loss function works(assuming mini_x of 3 samples) lossResults . tensor([31.4817], grad_fn=&lt;AddBackward0&gt;) . #to avoid underflow. Trying out basic code from the doc&#39;s here: loss = nn.CrossEntropyLoss() input = torch.randn(3, 5, requires_grad=True) target = torch.empty(3, dtype=torch.long).random_(5) output = loss(input, target) input,target . (tensor([[ 1.9199, -0.2254, -0.3417, 0.3040, -0.6890], [-1.1267, -0.2858, -1.0935, 1.1351, 0.7592], [-3.5945, 0.0192, 0.1052, 0.9603, -0.5672]], requires_grad=True), tensor([4, 1, 2])) . lossy=nn.CrossEntropyLoss() inp=myModel(mini_x) otpt=torch.cat(list(mini_y)) lossy(inp,otpt) . tensor(6.2963, grad_fn=&lt;NllLossBackward0&gt;) . . #The ideal loss function returns less loss if either confidence of right digit increases, or confidence #of wrong digit decreases. From this rational I get this: #Loss=sum(abs([model outputs for wrong digits]))/([Model output for right digit]) #This way, loss goes down as input for right digit goes up, and loss goes up as input for wrong digits go up def my_loss(preds,target): #Picked up a trick to place 0s at specfic indices in a tensor here:https://discuss.pytorch.org/t/creating-big-tensors-and-insert-ones-at-specific-dimensions/100025/2 #But it only works for 3D tensors and I couldnt find out how to generalize it to 2... so the following lines do some unsqueezing #To singleton upscale existing tensors to make this work. essentially getting a 0s/1s tensor with 1 at target indicies #to facilitate the loss calculation by isolating target values zeros=torch.zeros(preds.shape).unsqueeze(0) tgts=target.squeeze(1).unsqueeze(0) zeros[range(tgts.shape[0]), torch.tensor([range(tgts.shape[1])] * tgts.shape[0]).T, tgts.T] = 1 #Make target indices=1 targets_as_ones=zeros[0] #This retrieves the 2D tensor from the singleton 3D tensor targets_as_zeros=torch.where(targets_as_ones==0,1.,0.) targets=preds.unsqueeze(0)[range(tgts.shape[0]), torch.tensor([range(tgts.shape[1])] * tgts.shape[0]).T, tgts.T] #Pull out the NN output for the neuron #that represents the target value for that item. return abs(preds*targets_as_zeros/targets).sum() . my_loss(myModel(mini_x),mini_y) . tensor(31.4133, grad_fn=&lt;SumBackward0&gt;) . preds=myModel(mini_x) zeros=torch.zeros(preds.shape).unsqueeze(0) tgts=mini_y.squeeze(1).unsqueeze(0) zeros[range(tgts.shape[0]), torch.tensor([range(tgts.shape[1])] * tgts.shape[0]).T, tgts.T] = 1 #Make target indices=1 targets_as_ones=zeros[0] #This retrieves the 2D tensor from the singleton 3D tensor targets_as_zeros=torch.where(targets_as_ones==0,1.,0.) targets=preds.unsqueeze(0)[range(tgts.shape[0]), torch.tensor([range(tgts.shape[1])] * tgts.shape[0]).T, tgts.T] preds*targets_as_zeros,preds*targets_as_ones,preds.unsqueeze(0)[range(tgts.shape[0]), torch.tensor([range(tgts.shape[1])] * tgts.shape[0]).T, tgts.T] . (tensor([[-0.0000, 5.2352, 4.6036, -0.5644, 1.2204, 2.1069, -4.7518, -2.1475, 1.2007, 3.8679], [-0.0000, 3.2856, 2.8976, -0.2769, 0.8195, 1.3640, -2.8491, -1.2493, 0.8073, 2.4457], [-0.0000, 4.6264, 4.0708, -0.4746, 1.0952, 1.8748, -4.1576, -1.8670, 1.0779, 3.4238], [ 0.0000, -4.3303, -3.7665, 0.8462, -0.7469, -1.5381, 4.5836, 2.2592, -0.7293, -3.1099], [-0.0000, 2.1488, 1.9029, -0.1093, 0.5857, 0.9308, -1.7396, -0.7257, 0.5780, 1.6164]], grad_fn=&lt;MulBackward0&gt;), tensor([[-4.1185, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, -0.0000, -0.0000, 0.0000, 0.0000], [-2.4601, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, -0.0000, -0.0000, 0.0000, 0.0000], [-3.6006, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, -0.0000, -0.0000, 0.0000, 0.0000], [ 4.0184, -0.0000, -0.0000, 0.0000, -0.0000, -0.0000, 0.0000, 0.0000, -0.0000, -0.0000], [-1.4931, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, -0.0000, -0.0000, 0.0000, 0.0000]], grad_fn=&lt;MulBackward0&gt;), tensor([[-4.1185], [-2.4601], [-3.6006], [ 4.0184], [-1.4931]], grad_fn=&lt;IndexBackward0&gt;)) . abs(preds*targets_as_zeros/targets).sum() . tensor(31.4133, grad_fn=&lt;SumBackward0&gt;) . #Trying again but even simpler.. just maximize activation of target neuron def my_loss(preds,target): tgts=target.squeeze(1).unsqueeze(0) targets=preds.unsqueeze(0)[range(tgts.shape[0]), torch.tensor([range(tgts.shape[1])] * tgts.shape[0]).T, tgts.T] #Pull out the NN output for the neuron return abs(1/targets).sum() #Invert them because we want to minimize loss. So the larger the absolute activation, smaller loss . #Need to incorporate cost of being wrong. This one tries to do so by returning the average ratio of #wrong activation vs right activation. Notice that activation are not abs() in taking ratio.. #I think that allowing activation of wrong value to continue to be forced down into negative dimension gives more #freedom for learning... Because there is more room on interval of [-inf,+inf] than [0,+inf], so more #room for differentiating prediction to capitalize on def my_loss(preds,target): tgts=target.squeeze(1).unsqueeze(0) targets=preds.unsqueeze(0)[range(tgts.shape[0]), torch.tensor([range(tgts.shape[1])] * tgts.shape[0]).T, tgts.T] #Pull out the NN output for the neuron return (preds/targets).mean() . #and found perhaps I shouldve used the cross entropy function as my loss function... #=-log(softmax value of correct activation unit) . def my_loss(preds,target): loss_fn=nn.CrossEntropyLoss() tgts=target.view(-1)#Turn tensor of singleton tensors one per target into a single tensor with all as elements return loss_fn(preds,tgts) #Nice . def batch_accuracy(mdl,xb, yb): otpts = mdl(xb) #Get output activations from model preds= otpts.max(dim=-1).indices #The indices of the max activation is the predicted digit of the input correct=preds==yb.view(-1) #Types must be tensors to return sequence of true/false # Use view to take it from shape=[5,1] to [5], same as preds. else will broadcast and end result all messed up return correct.float().mean() def validate_epoch(mdl): outcomes=[batch_accuracy(mdl,xb,yb) for xb,yb in valid_dl] return round(torch.stack(outcomes).mean().item(),4) . #This is to valdiate that we&#39;re pulling the right data, so as to validate the validate_epoch function validIter=iter(dls[1]) xv,yv=next(validIter) for i in range(len(xv)): show_image(xv[i].view((8,8))) print(yv[i].data.item()) . 9 0 6 6 9 . #batch_accuracy otuput to validate its correct validIter=iter(dls[1]) xv,yv=next(validIter) o=myModel(xv) print(o.max(dim=-1).indices,yv,batch_accuracy(myModel,xv,yv)) . tensor([1, 1, 6, 1, 1]) tensor([[3], [7], [0], [1], [4]]) tensor(0.1600) . o . tensor([[-6.1172, 7.5849, 6.6596, -0.9109, 1.7037, 3.0022, -7.0449, -3.2300, 1.6748, 5.5819], [-8.4815, 10.3643, 9.0917, -1.3208, 2.2753, 4.0613, -9.7575, -4.5104, 2.2356, 7.6094], [ 0.4390, -0.1225, -0.0845, 0.2257, 0.1185, 0.0653, 0.4770, 0.3207, 0.1197, -0.0404], [-5.2695, 6.5883, 5.7876, -0.7640, 1.4987, 2.6225, -6.0723, -2.7709, 1.4737, 4.8550], [-4.3495, 5.5068, 4.8412, -0.6045, 1.2763, 2.2103, -5.0168, -2.2726, 1.2555, 4.0660]], grad_fn=&lt;AddBackward0&gt;) . myModel(mini_x),mini_y,batch_accuracy(myModel,mini_x,mini_y),validate_epoch(myModel) #validate the function works right... #since train_y and _x were organized with digits in sequence (0&#39;s, then 1&#39;s, etc...), the first 5 digits #being tested here are all 0&#39;s. Observe that if 1/5 outputs max value is in 0 index, then the accuracy is 0.2 #Might take initializing new parameters multiple times to get a correct guess, it will be random #Given the relu activation, the model may yield multiple identical values for multiple guesses/vectors when relu activation #was zero. In this case, the max function returns the 0 index, and 0 is the target, so 0&#39;s will be right. #tbh i think this is a weakness with this model, and having a different randomly initialized bias for each feature would #eliminate this and be better . (tensor([[-4.1185, 5.2352, 4.6036, -0.5644, 1.2204, 2.1069, -4.7518, -2.1475, 1.2007, 3.8679], [-2.4601, 3.2856, 2.8976, -0.2769, 0.8195, 1.3640, -2.8491, -1.2493, 0.8073, 2.4457], [-3.6006, 4.6264, 4.0708, -0.4746, 1.0952, 1.8748, -4.1576, -1.8670, 1.0779, 3.4238], [ 4.0184, -4.3303, -3.7665, 0.8462, -0.7469, -1.5381, 4.5836, 2.2592, -0.7293, -3.1099], [-1.4931, 2.1488, 1.9029, -0.1093, 0.5857, 0.9308, -1.7396, -0.7257, 0.5780, 1.6164]], grad_fn=&lt;AddBackward0&gt;), tensor([[0], [0], [0], [0], [0]]), tensor(0.), 0.115) . testLoader=iter(dls[0]) . lr=0.01 xb,yb=next(testLoader) loss=my_loss(myModel(xb),yb) loss.backward() with torch.no_grad(): for p in w1,b1,w2,b2: p.data= p.data-p.grad.data*lr p.grad.zero_() print(loss) . tensor(8.6665, grad_fn=&lt;NllLossBackward0&gt;) . if type(w1.grad)==NoneType: print(tensor([(x.data.mean()) for x in [w1,b1,w2,b2]])) else: print(tensor([(x.data.mean(),x.grad.data.mean()) for x in [w1,b1,w2,b2]])) for p in w1,b1,w2,b2: p.grad.zero_() . tensor([[ 0.0036, 0.0000], [ 0.0641, 0.0000], [-0.0487, 0.0000], [ 0.1809, 0.0000]]) . #Running this one and then cell below will allow to observe changes in the parameters #as a result of back propagation and gradient descent! #If change to parameters is not observed, run cell below to verify non zero gradient is being computed lr=0.01 for xb,yb in dls[0]: loss=my_loss(myModel(xb),yb) loss.backward() with torch.no_grad(): for p in w1,b1,w2,b2: p.data= p.data-p.grad.data*lr p.grad.zero_() print(loss) #Prints loss for each minibatch . tensor(7.8955, grad_fn=&lt;NllLossBackward0&gt;) tensor(5.5976, grad_fn=&lt;NllLossBackward0&gt;) tensor(6.0172, grad_fn=&lt;NllLossBackward0&gt;) tensor(5.6406, grad_fn=&lt;NllLossBackward0&gt;) tensor(5.9263, grad_fn=&lt;NllLossBackward0&gt;) tensor(5.2161, grad_fn=&lt;NllLossBackward0&gt;) tensor(4.9269, grad_fn=&lt;NllLossBackward0&gt;) tensor(4.4739, grad_fn=&lt;NllLossBackward0&gt;) tensor(4.7216, grad_fn=&lt;NllLossBackward0&gt;) tensor(3.8051, grad_fn=&lt;NllLossBackward0&gt;) tensor(3.8510, grad_fn=&lt;NllLossBackward0&gt;) tensor(3.0392, grad_fn=&lt;NllLossBackward0&gt;) tensor(3.1438, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.8495, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.6098, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.7949, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.6347, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.4974, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.3163, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.2406, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.1818, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.3660, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.2306, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.2963, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.0940, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.2297, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.2294, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.1313, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.2549, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.1769, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.1987, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.2986, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.2026, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.1892, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.3056, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.1602, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.0594, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.3139, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.2606, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.0796, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.0918, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.1626, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.1314, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.1663, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.1291, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.2918, grad_fn=&lt;NllLossBackward0&gt;) tensor(1.9681, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.0553, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.2321, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.0318, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.0983, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.4087, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.1333, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.2700, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.2204, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.0809, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.1463, grad_fn=&lt;NllLossBackward0&gt;) tensor(1.9678, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.1310, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.0630, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.1422, grad_fn=&lt;NllLossBackward0&gt;) tensor(2.2210, grad_fn=&lt;NllLossBackward0&gt;) . #(Running this one and then the above will allow for verification/display of computation result of non zero gradients) def calc_grad(xb, yb, model,f_loss): preds = model(xb) loss = f_loss(preds, yb) loss.backward() return loss.data.item() #Return the loss (see why later) calc_grad(mini_x,mini_y,myModel,my_loss) . 2.0894858837127686 . #Running this then the print cell 2 above will allow for validation of code by observing parameter change. If no change, #Run one above to check gradient size def train_epoch(model,lr,params,f_loss): for xb,yb in dls[0]: calc_grad(xb,yb,model,f_loss) with torch.no_grad(): for p in w1,b1,w2,b2: p.data= p.data-p.grad.data*lr p.grad.zero_() train_epoch(myModel,0.01,[w1,b1,w2,b2],my_loss) . class BasicOptim: def __init__(self,params,lr): self.params,self.lr = list(params),lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None . def train_epoch(model,opt,lr,params,f_loss): losses=[]#Will allow for recording epoch wise loss for xb,yb in dls[0]: calc_grad(xb,yb,model,f_loss) opt.step() losses.append(calc_grad(xb,yb,model,f_loss)) opt.zero_grad() return tensor(losses) #Will allow for analyzing performance w.r.t loss as the model learns . my_opt=BasicOptim([w1,b1,w2,b2],0.01) res=train_epoch(myModel,my_opt,0.01,[w1,b1,w2,b2],my_loss) res.mean(),res . (tensor(2.0317), tensor([2.0741, 1.7660, 2.2313, 1.9984, 1.8884, 2.1686, 1.7515, 2.0124, 1.9008, 2.1306, 2.1752, 1.9439, 1.9268, 1.9860, 2.1010, 2.1853, 2.2638, 2.2290, 1.9893, 1.9734, 1.9442, 2.0295, 2.1597, 2.0376, 2.0211, 2.0450, 1.9879, 2.1127, 1.8709, 1.9576, 2.1561, 1.9560, 2.1379, 2.0640, 2.0666, 1.9608, 2.0936, 2.2478, 2.1695, 2.0455, 1.8293, 2.0236, 2.0528, 2.0663, 2.1181, 1.8855, 2.1618, 2.0844, 1.7411, 1.9947, 1.8307, 1.8846, 1.9199, 1.9247, 1.9935, 2.1115, 2.2204, 2.1065, 2.1304, 1.9570, 1.9483, 2.2220])) . class cTrial: def __init__(self,numE=10,lr=0.01,model=myModel,opt=my_opt,params=[w1,b1,w2,b2],f_loss=my_loss): self.numE=numE self.lr=lr self.model=model self.opt=opt self.params=params self.f_loss=f_loss self.res=[] self.valids=[] self.wtsHist=[] # For tracking change in weights across learning def run(self,numE=None,wkLr=None): self.valids=[] #Reset epch_losses=[] #Reset self.wtsHist=[[],[],[],[]] # 4 contents, w1,b1,w2,b2 if numE is None: numE=self.numE if wkLr is None: wkLr=self.lr for i in range(numE): #-- Record wts for analysis self.wtsHist[0].append(list(x.item() for x in w1.data.view(-1))) self.wtsHist[1].append(list(x.item() for x in b1.data.view(-1))) self.wtsHist[2].append(list(x.item() for x in w2.data.view(-1))) self.wtsHist[3].append(list(x.item() for x in b2.data.view(-1))) #-- res=train_epoch(self.model,self.opt,self.lr,self.params,self.f_loss) epch_losses.append(res) self.valids.append(validate_epoch(self.model)) self.res=torch.stack(epch_losses) self.valids=tensor(self.valids) #self.wtsHist=[tensor([self.wtsHist[0],self.wtsHist[2]]),tensor([self.wtsHist[1],self.wtsHist[3]])] #self.res=epch_losses . #Leaving off with the below cell failing and I can&#39;t really imagine why... #Also the basic &#39;activation maximizer&#39; is at risk of naively increasing ALL activations #Empirically after many epochs it just chooses 1 digit for everything even as loss goes down. #Need to somehow include cost of being wrong agian.. Maybe sum the average of wrong activation/tgt activation! . #Leaving off having found that commenting out the ReLu layer stops the behaviour where the gradient was zeroing out on w1,b1,w2 #Also implemented the &#39;average ratio of wrong activation against right one&#39; loss function #Still observing that the NN is resorting to naively estimating 1 digit, resulting in constant 10% sucess rate... #Was working on a function to plot the weight changes across epochs to monitor for wierd behaviour.. #basically cast the list output as a tensor.. iterate through *columns* with .view()to get parameter wise data across iterations . my_try=cTrial() my_try.run(numE=50,wkLr=0.001) my_try.res.shape #We see the result is a tensor of numE tensors of 62 results. We initiated the dataLoaders #with 25 items to a minibatch, with 1540 data samples across all inputs, 25*62=1550, so we have 61 minibatches of 25 with #one last minibatch of 15 . torch.Size([50, 62]) . my_try.res.mean(dim=1) #Return average loss across all minibatches in each epoch, in chronological sequence. #If loss is going down... SUCCESS!!! (or at least its working as intended!) . tensor([2.0169, 1.9830, 1.9666, 1.9293, 1.9145, 1.8770, 1.8447, 1.8138, 1.7978, 1.7788, 1.7664, 1.7587, 1.7477, 1.7338, 1.7375, 1.7188, 1.7214, 1.7153, 1.7065, 1.6975, 1.7047, 1.6953, 1.6931, 1.6909, 1.6839, 1.6845, 1.6900, 1.6598, 1.6593, 1.6518, 1.6599, 1.6535, 1.6382, 1.6410, 1.6291, 1.6362, 1.6249, 1.6178, 1.6197, 1.6216, 1.6137, 1.6149, 1.6176, 1.6130, 1.6013, 1.6048, 1.6106, 1.6056, 1.6002, 1.5979]) . plt.plot(my_try.res.mean(dim=1)) . [&lt;matplotlib.lines.Line2D at 0x7f19f8e3eb50&gt;] . plt.plot(my_try.valids) . [&lt;matplotlib.lines.Line2D at 0x7f19f9798dd0&gt;] . #What I&#39;m observing is the model always settles on choosing one of 2 possible digits, #(seemingly random based on initial condition), so its performance still sucks. #Wondering: What if it needs greater capacity? Greater layers? #-&gt; going to redefine model.. . W1=init_params((8*8,32)) #64 in, 32 out B1=init_params(32) # Feature-specific biases added W2=init_params((32,16)) # 32 in, 16 out B2=init_params(16) # Feature-specific biases added W3=init_params((16,10)) # 16 in, 10 out B3=init_params(10) # Feature-specific biases added . def mdlV2(xb): res= xb@W1+B1 res=res.max(tensor(0.)) res=res@W2+B2#returns 10 features for each input res=res.max(tensor(0.)) res=res@W3+B3#returns 10 features for each input return res . mdlV2(mini_x) #Test with random data sample to see the matrix multiplication works, parameters defined correctly... . tensor([[ 1.0679, -1.8199, 0.4861, -0.6773, -0.4341, -0.0244, -1.4107, 0.5197, 0.7869, 0.7355], [ 1.2566, -2.1620, 0.7953, -0.5469, -0.4030, -0.2573, -1.2032, 0.9089, 0.6863, 0.8816], [ 1.6671, -2.5187, 0.7505, -0.6515, -0.0254, 0.0655, -1.9130, 0.6849, 0.3266, 0.2800], [ 1.3645, -2.4120, 0.5461, -0.7007, -0.4227, -0.1179, -1.6952, 0.9528, 0.6045, 1.0226], [ 1.7769, -2.7423, 0.7523, -0.8904, -0.1210, 0.1373, -2.2262, 0.8818, 0.4147, 0.6171]], grad_fn=&lt;AddBackward0&gt;) . my_try=cTrial(model=mdlV2,opt=BasicOptim([W1,B1,W2,B2,W3,B3],0.01),params=[W1,B1,W2,B2,W3,B3]) my_try.run(numE=20,wkLr=20) . plt.plot(my_try.res.mean(dim=1)[1:]) . [&lt;matplotlib.lines.Line2D at 0x7f19f3563610&gt;] . plt.plot(my_try.valids) . [&lt;matplotlib.lines.Line2D at 0x7f19f34de510&gt;] . #to get a sense for the outputs . def viewSamplePredictions(model): validIter=iter(dls[1]) xv,yv=next(validIter) o=model(xv) print(o.max(dim=-1).indices,yv,batch_accuracy(model,xv,yv)) viewSamplePredictions(mdlV2) . tensor([4, 4, 4, 4, 4]) tensor([[2], [7], [0], [2], [1]]) tensor(0.) . def confMtx(model): conf=torch.zeros([10, 10],dtype=torch.int32) for xv,yv in dls[1]: preds=model(xv).max(dim=-1).indices for i in range(len(xv)): conf[yv[i].item()][preds[i].item()]+=1 df = pd.DataFrame(conf) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) return df.style.set_caption(&quot;Top Axis: Predicted value. Left Axis: Actual Value&quot;) #return df confMtx(mdlV2) . Top Axis: Predicted value. Left Axis: Actual Value &nbsp; 0 1 2 3 4 5 6 7 8 9 . 0 18 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | . 1 1 | 0 | 0 | 0 | 0 | 11 | 0 | 6 | 0 | 2 | . 2 0 | 0 | 0 | 11 | 0 | 0 | 0 | 0 | 0 | 9 | . 3 0 | 0 | 0 | 18 | 0 | 0 | 0 | 0 | 0 | 2 | . 4 4 | 0 | 0 | 0 | 0 | 0 | 13 | 2 | 0 | 1 | . 5 3 | 0 | 0 | 0 | 0 | 14 | 0 | 2 | 0 | 1 | . 6 9 | 0 | 0 | 0 | 0 | 1 | 10 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 7 | 0 | 2 | 0 | 2 | 0 | 9 | . 8 3 | 0 | 0 | 0 | 0 | 12 | 0 | 0 | 0 | 5 | . 9 0 | 0 | 0 | 5 | 0 | 4 | 0 | 1 | 0 | 10 | . #like mothers milk. But now its output space just lacks 0,3,7! Going to add greater capacity... #Funnel shaped NN . W1v3=init_params((8*8,48)) #64 in, 32 out B1v3=init_params(48) # Feature-specific biases added W2v3=init_params((48,32)) # 32 in, 16 out B2v3=init_params(32) # Feature-specific biases added W3v3=init_params((32,24)) # 16 in, 10 out B3v3=init_params(24) # Feature-specific biases added W4v3=init_params((24,18)) # 16 in, 10 out B4v3=init_params(18) W5v3=init_params((18,14)) B5v3=init_params(14) W6v3=init_params((14,10)) B6v3=init_params(10) . def mdlV3(xb): res= xb@W1v3+B1v3 res=res.max(tensor(0.)) res=res@W2v3+B2v3 res=res.max(tensor(0.)) res=res@W3v3+B3v3 res=res.max(tensor(0.)) res=res@W4v3+B4v3 res=res.max(tensor(0.)) res=res@W5v3+B5v3 res=res.max(tensor(0.)) res=res@W6v3+B6v3 return res . mdlV3(mini_x) . tensor([[ 345.8418, 289.0138, 60.7753, 223.0188, 113.1826, 104.4974, 336.0727, -771.3594, 334.1454, 381.0231], [ 210.8918, 151.5357, 136.3566, 221.9274, 72.0100, 221.6852, 366.7518, -573.8847, 342.6361, 68.7711], [ 509.5496, 318.0272, 156.9736, 234.6567, 102.3715, 238.3482, 468.9347, -909.8787, 501.5280, 275.4095], [ 318.9915, 458.8397, -21.3896, 186.9061, 23.5667, -189.7860, 74.7113, -610.5369, 152.0016, 583.3962], [ 353.9034, 382.0305, 125.8823, 263.3908, 93.5603, 96.6531, 309.1708, -652.4356, 318.3333, 234.5244]], grad_fn=&lt;AddBackward0&gt;) . my_try=cTrial(model=mdlV3,opt=BasicOptim([W1v3,B1v3,W2v3,B2v3,W3v3,B3v3,W4v3,B4v3,W5v3,B5v3,W6v3,B6v3],0.01),params=[W1v3,B1v3,W2v3,B2v3,W3v3,B3v3,W4v3,B4v3,W5v3,B5v3,W6v3,B6v3]) my_try.run(numE=100,wkLr=0.001) . plt.plot(my_try.res.mean(dim=1)[1:]) #Omit first result, makes end values not discriminable . [&lt;matplotlib.lines.Line2D at 0x7f19f9d83450&gt;] . plt.plot(my_try.valids) . [&lt;matplotlib.lines.Line2D at 0x7f19fa9df190&gt;] . viewSamplePredictions(mdlV3) . tensor([0, 0, 0, 0, 0]) tensor([[8], [0], [9], [2], [1]]) tensor(0.2000) . confMtx(mdlV3) . #Instead of funnel shape, Ill make it wormhole shape (trumpet bell.. start wide, go narrow fast but long) . W1v4=init_params((8*8,32)) #64 in, 32 out B1v4=init_params(32) # Feature-specific biases added W2v4=init_params((32,16)) # 32 in, 16 out B2v4=init_params(16) # Feature-specific biases added W3v4=init_params((16,10)) # 16 in, 10 out B3v4=init_params(10) # Feature-specific biases added W4v4=init_params((10,10)) # 16 in, 10 out B4v4=init_params(10) W5v4=init_params((10,10)) B5v4=init_params(10) W6v4=init_params((10,10)) B6v4=init_params(10) W7v4=init_params((10,10)) B7v4=init_params(10) W8v4=init_params((10,10)) B8v4=init_params(10) W9v4=init_params((10,10)) B9v4=init_params(10) W10v4=init_params((10,10)) B10v4=init_params(10) W11v4=init_params((10,10)) B11v4=init_params(10) W12v4=init_params((10,10)) B12v4=init_params(10) . def mdlV4(xb): res= xb@W1v4+B1v4 res=res.max(tensor(0.)) res=res@W2v4+B2v4 res=res.max(tensor(0.)) res=res@W3v4+B3v4 res=res.max(tensor(0.)) res=res@W4v4+B4v4 res=res.max(tensor(0.)) res=res@W5v4+B5v4 res=res.max(tensor(0.)) res=res@W6v4+B6v4 res=res.max(tensor(0.)) res=res@W7v4+B7v4 res=res.max(tensor(0.)) res=res@W8v4+B8v4 res=res.max(tensor(0.)) res=res@W9v4+B9v4 res=res.max(tensor(0.)) res=res@W10v4+B10v4 res=res.max(tensor(0.)) res=res@W11v4+B11v4 res=res.max(tensor(0.)) res=res@W12v4+B12v4 return res . mdlV4(mini_x)#Checking math works . tensor([[ 7.4950e+01, 1.7422e+02, -2.3696e+01, -3.5641e+01, 4.9170e+01, 1.5804e+02, 1.5012e+01, -1.2010e+02, 1.6857e+01, -5.1689e+01], [ 2.5860e+01, 5.2283e+01, -1.9358e+01, -2.6612e+01, 2.3746e+01, 3.8518e+01, 1.3859e-01, -3.9123e+01, -2.9411e+00, -4.1714e+01], [ 2.5161e+01, 6.0673e+01, -1.2859e+01, -2.2634e+01, 1.7738e+01, 5.2163e+01, 1.5605e+00, -4.2871e+01, -6.3931e-02, -2.9956e+01], [ 9.5547e+01, 2.3226e+02, -3.0140e+01, -5.3980e+01, 5.9029e+01, 2.2473e+02, 1.4448e+01, -1.5792e+02, 1.8842e+01, -7.1073e+01], [ 8.1295e+01, 1.9916e+02, -2.5350e+01, -4.7145e+01, 4.9380e+01, 1.9437e+02, 1.1905e+01, -1.3606e+02, 1.6377e+01, -6.0936e+01]], grad_fn=&lt;AddBackward0&gt;) . my_try=cTrial(model=mdlV4,opt=BasicOptim([W1v4,B1v4,W2v4,B2v4,W3v4,B3v4,W4v4,B4v4,W5v4,B5v4,W6v4,B6v4,W7v4,B7v4,W8v4,B8v4,W9v4,B9v4,W10v4,B10v4,W11v4,B11v4,W12v4,B12v4],0.01),params=[W1v4,B1v4,W2v4,B2v4,W3v4,B3v4,W4v4,B4v4,W5v4,B5v4,W6v4,B6v4,W7v4,B7v4,W8v4,B8v4,W9v4,B9v4,W10v4,B10v4,W11v4,B11v4,W12v4,B12v4]) my_try.run(numE=20,wkLr=10) . #With the multiple layers of multiplication... reducing variance of initial param random sampling mitigates this.. plt.plot(my_try.res.mean(dim=1)[1:]) #Omit first result, makes end values not discriminable . [&lt;matplotlib.lines.Line2D at 0x7f19f346e210&gt;] . plt.plot(my_try.valids) . [&lt;matplotlib.lines.Line2D at 0x7f19f33e3990&gt;] . viewSamplePredictions(mdlV4) . tensor([0, 0, 0, 0, 0]) tensor([[9], [5], [5], [6], [0]]) tensor(0.2000) . confMtx(mdlV4) . Top Axis: Predicted value. Left Axis: Actual Value &nbsp; 0 1 2 3 4 5 6 7 8 9 . 0 20 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 20 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 20 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 20 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 20 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 20 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 20 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 7 20 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 8 20 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 9 20 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . #Finding that training across just a few epochs, the model is learning to take a stab at all numbers! . #bad random starting place, then reduce over time to hone in on a local optimum... Can try this?... #Defining new function (alternatively could be made a method of Learner): def shrinkSteps(lrnr,numE,lrHi,lrLo): for lr in np.linspace(lrHi,lrLo,numE): lrnr.run(numE=1,wkLr=lr) . my_try=cTrial(model=mdlV4,opt=BasicOptim([W1v4,B1v4,W2v4,B2v4,W3v4,B3v4,W4v4,B4v4,W5v4,B5v4,W6v4,B6v4,W7v4,B7v4,W8v4,B8v4,W9v4,B9v4,W10v4,B10v4,W11v4,B11v4,W12v4,B12v4],0.01),params=[W1v4,B1v4,W2v4,B2v4,W3v4,B3v4,W4v4,B4v4,W5v4,B5v4,W6v4,B6v4,W7v4,B7v4,W8v4,B8v4,W9v4,B9v4,W10v4,B10v4,W11v4,B11v4,W12v4,B12v4]) shrinkSteps(my_try,8,.5,.01) #Unfortunately still naive at a broad range of values . #Now trying a needle format NN . W1v4=init_params((8*8,10)) #64 in, 10 out B1v4=init_params(1) W2v4=init_params((10,10)) B2v4=init_params(1) W3v4=init_params((10,10)) B3v4=init_params(1) W4v4=init_params((10,10)) B4v4=init_params(1) W5v4=init_params((10,10)) B5v4=init_params(1) W6v4=init_params((10,10)) B6v4=init_params(1) W7v4=init_params((10,10)) B7v4=init_params(1) W8v4=init_params((10,10)) B8v4=init_params(1) W9v4=init_params((10,10)) B9v4=init_params(1) W10v4=init_params((10,10)) B10v4=init_params(1) W11v4=init_params((10,10)) B11v4=init_params(1) W12v4=init_params((10,10)) B12v4=init_params(1) . #(Also fixed an error in how batch accuracy was computed...) #It turns out the size of parameters when initialized also has a huge impact on outcomes #Depending on shape of NN, large params can lead to naive selection of only a single digit that then never learns #I had reduced standard dev of initialization distribution when experimenting with later models, but this proved #just the trick for mdlV2! Achieved 93% digit recognition on the validation set! :) .",
            "url": "https://davidd003.github.io/Posts/fastai/jupyter/2022/08/02/Building-A-Digit-Classifier.html",
            "relUrl": "/fastai/jupyter/2022/08/02/Building-A-Digit-Classifier.html",
            "date": " • Aug 2, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://davidd003.github.io/Posts/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://davidd003.github.io/Posts/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a Torontonian in London (Ontario): an avid (rabid) consumer of podcasts, interested in music and AI. . Fun fact! This website is powered by fastpages 1. I would encourage you tot ry it out. It’s free to host, easy to update, flexible, and fun to learn! . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://davidd003.github.io/Posts/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://davidd003.github.io/Posts/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}